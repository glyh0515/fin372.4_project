{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Data Processor</span> for Book-to-Market Strategy\n",
    "\n",
    "This sheet starts the process of coding a backtest for a strategy picking stocks based on the ratio of the book value of equity to the market value of equity. The first step is to load data on both accounting numbers and stock returns, and organize it so we can access it as we backtest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processor required functions\n",
    "\n",
    "According to the pseudo-code above, the <span style=\"color:green\">Data Processor</span> needs to do the following tasks:\n",
    "1. Load all the necessary raw data (in constructor)\n",
    "1. Return an array of unique dates in the raw data (`unique_dates()`)\n",
    "1. For a given date, return a signal DataFrame containing all the latest signals for the appropriate universe of securities (`signal_df_for_date(date)`)\n",
    "1. For a given date, return a price DataFrame containing the latest prices for all securities potentially in the portfolio, including those not in the current investable universe (`price_df_for_date(date)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class here\n",
    "class BMDataProcessor():\n",
    "    \n",
    "    # Path to where we store the data\n",
    "    data_folder_path = Path('C:/Dropbox/Jupyter notebooks/Backtest Demo/Data/') \n",
    "    \n",
    "    # Number of days between quarterly earnings announcement and when we can use data\n",
    "    min_accounting_lag = 30\n",
    "    max_accounting_lag = 365\n",
    "    \n",
    "    # Minimum share price to open a new position\n",
    "    min_share_price = 3.0\n",
    "    \n",
    "    # Constructor, loads/cleans/merges data as needed\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Load price data: monthly 1961-2020 sample from CRSP including all public US equities\n",
    "        # In monthly_returns.csv\n",
    "        self.price_df = pd.read_csv(self.data_folder_path / 'monthly_returns.csv')\n",
    "        \n",
    "        # Parse the yyyyMMdd int dates into DateTime64\n",
    "        # Based on formatting strings here\n",
    "        # https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "        self.price_df['date'] = pd.to_datetime(self.price_df.loc[:,'date'], format =\"%Y%m%d\")\n",
    "        \n",
    "        # Prices sometimes negative to indicate no volume at closing auction\n",
    "        # In these cases, price = -0.5*(bid+ask)\n",
    "        # But we don't use that information and so want prices to always be positive\n",
    "        # See http://www.crsp.org/products/documentation/data-definitions-p\n",
    "        self.price_df.loc[:,'prc'] = np.absolute(self.price_df.loc[:,'prc'])\n",
    "        \n",
    "        # Add next-months return as a new column 'ret_next'\n",
    "        # Use the safe_lead_lag: want lead return but only when permno the same\n",
    "        self.price_df.loc[:,'ret_next'] = safe_lead_lag(self.price_df.loc[:,'ret'],self.price_df.loc[:,'permno'],1)\n",
    "        \n",
    "        # Load accounting data used for BM signal\n",
    "        # Quarterly sample from 1961-2020 from Compustat Fundamentals Quarterly\n",
    "        # Stored in the `comp_bm.dta` file\n",
    "        # `.dta` files are Stata data, and do a better job than `.csv` files of remembering data types\n",
    "        self.signal_df = pd.read_stata(self.data_folder_path / 'comp_bm.dta')\n",
    "        \n",
    "        # The problem with our accounting data is that it identifies stocks using gvkey instead of permno\n",
    "        # To merge with return_df, we need to use another dataset that converts gvkey to permno\n",
    "        # This is stored in the gvkey_permno_conversion.dta file\n",
    "        self.gvkey_permno_conversion = pd.read_stata(self.data_folder_path / 'gvkey_permno_conversion.dta')\n",
    "\n",
    "        # Use a merge command to add the permno column to our signal_df\n",
    "        self.signal_df = self.signal_df.merge(self.gvkey_permno_conversion,on=['gvkey','datadate'])           \n",
    "        \n",
    "    # Returns an array with the unique dates for which we have loaded data\n",
    "    # Uses from the price_df since that's how frequency we can update portfolio value\n",
    "    # Filters all dates in price_df to return only dates for which we have signals as well\n",
    "    def unique_dates(self):\n",
    "        price_dates = pd.Series( np.sort(self.price_df.loc[:,'date'].unique()) )\n",
    "        min_signal_date = self.signal_df.loc[:,'rdq'].min() + np.timedelta64(self.min_accounting_lag,'D')\n",
    "        max_signal_date =  self.signal_df.loc[:,'rdq'].max() + np.timedelta64(self.max_accounting_lag,'D')\n",
    "        return price_dates[ (price_dates >= min_signal_date) & (price_dates <= max_signal_date) ].array\n",
    "    \n",
    "    # Returns a DataFrame containing one row for all securities in price_df as of date.\n",
    "    # Columns must include:\n",
    "    # - 'date': date on which price data observed\n",
    "    # - 'security_id': a security identifier\n",
    "    # - 'prc': price on date\n",
    "    # - 'ret': return from previous date to date\n",
    "    # Ignores liquidity and future-return availability requirements\n",
    "    # To be used only for closing decisions and execution decisions\n",
    "    # Some of the returned stocks cannot be traded\n",
    "    def price_df_for_date(self,date):\n",
    "        price_date_df = self.price_df.loc[ self.price_df.loc[:,'date'] == date, :]\n",
    "        return price_date_df.rename(columns={'permno':'security_id'}) \n",
    "    \n",
    "    # Returns a DataFrame where each row is a security in the strategy's universe,\n",
    "    # Columes must include:\n",
    "    # - 'date': date on which price data observed\n",
    "    # - 'security_id': a security identifier\n",
    "    # - whatever signals the trading rule needs to decide which securities to open new positions in\n",
    "    #   - In this case, return cshoq, prccq, and ceqq so trading rule can compute B/M ratio\n",
    "    #\n",
    "    # Also responsible for applying whatever liquidity filters are wanted to narrow universe,\n",
    "    # and check that we have future return data (no point in backtesting if we don't know what happens next)\n",
    "    def signal_df_for_date(self,date):\n",
    "        # find set of permnos considered tradeable as of date \n",
    "                \n",
    "        # start with all rows return_df on date with non-nan and non-infinite ret_next\n",
    "        date_price_df = self.price_df.loc[ self.price_df.loc[:,'date'] == date,:]\n",
    "        date_price_df = date_price_df.loc[ np.isfinite(self.price_df.loc[:,'ret_next']),:]\n",
    "        date_price_df = date_price_df.drop(columns=['ret_next'])\n",
    "        \n",
    "        # now signal data\n",
    "        # first only look at data announced at least accounting_lag_days prior to date\n",
    "        all_past_signal_df = self.signal_df.loc[(self.signal_df.loc[:,'rdq'] < date - np.timedelta64(self.min_accounting_lag,'D')),:]\n",
    "        # then grab only the latest observation for each permno\n",
    "        latest_signal_df = all_past_signal_df.groupby('permno').last()\n",
    "        \n",
    "        # now merge with return data and return        \n",
    "        merged_df = date_price_df.merge(latest_signal_df,on='permno',how='inner')\n",
    "        merged_df = merged_df.rename(columns={'permno':'security_id'})  # use permno as our security_id\n",
    "        \n",
    "        # filter by liquidity requirements\n",
    "        merged_df = self.liquidity_filter(merged_df)\n",
    "        \n",
    "        # and return without the ret_next column so backtests don't cheat by using it\n",
    "        return merged_df\n",
    "\n",
    "    # Returns a filterd version of the passed DataFrame,\n",
    "    # with all observations deemed too illiquid removed\n",
    "    # Liquidity requirements:\n",
    "    #  - price >= $3\n",
    "    def liquidity_filter(self,df):\n",
    "        return df.loc[ df.loc[:,'prc'] >= self.min_share_price,:]\n",
    "        \n",
    "###################################################################\n",
    "# Helper methods, do not modify\n",
    "###################################################################\n",
    "\n",
    "# Function safe_lead_lag returns a new Series with the lead/lagged values\n",
    "#  but only when a group is the same for the lead/lag\n",
    "# Inputs:\n",
    "# - data_series: data we want to lead/lag\n",
    "# - group_series: grouping we want to be the same for the lead/lag to be value\n",
    "# requires data_series and group_series already by sorted by group_series\n",
    "# so that all alike values of group_series are adjacent,\n",
    "# meaning group_series should look like:\n",
    "#    g_0\n",
    "#    g_0\n",
    "#    g_0\n",
    "#    g_0\n",
    "#    g_1\n",
    "#    g_1\n",
    "#    g_2\n",
    "#    g_2 \n",
    "#    ...\n",
    "# where g_i indicates the observation is in group i,\n",
    "# and once the first g_{i+1} appears no more g_i values appear\n",
    "# \n",
    "# lead_lag > 0 returns a data_series with values of data_series lead_lag rows ahead\n",
    "# as long as group_series remains the same, NaN if group different\n",
    "# lead_lag < 0 returns a data_series with values of data_series -lead_lag rows behind \n",
    "# (same as lead_lag rows ahead) as long as group_series remains the same, NaN if group different\n",
    "def safe_lead_lag(data_series,group_series,lead_lag): \n",
    "    df = pd.DataFrame({ 'data': data_series, 'group': group_series })\n",
    "    return df.groupby(['group'])['data'].shift(-lead_lag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
